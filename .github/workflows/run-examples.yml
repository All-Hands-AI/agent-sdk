---
name: Run Examples Scripts on PR

on:
    pull_request:
        types: [labeled]

permissions:
    contents: read
    pull-requests: write

jobs:
    test-examples:
        if: github.event.label.name == 'test-examples'
        runs-on: blacksmith-2vcpu-ubuntu-2404
        timeout-minutes: 60
        steps:
            - name: Checkout
              uses: actions/checkout@v5

            - name: Install uv
              uses: astral-sh/setup-uv@v7
              with:
                  enable-cache: true

            - name: Install Node.js
              uses: actions/setup-node@v4
              with:
                  node-version: '22'

            - name: Install dependencies
              run: uv sync --frozen --group dev

            - name: Run examples
              env:
                  LLM_API_KEY: ${{ secrets.LLM_API_KEY }}
                  LLM_MODEL: openhands/claude-haiku-4-5-20251001
                  LLM_BASE_URL: ${{ secrets.LLM_BASE_URL }}
              run: |
                  # List of examples to test
                  # Excluded examples:
                  # - 04_confirmation_mode_example.py: requires user input
                  # - 06_interactive_terminal_w_reasoning.py: interactive terminal
                  # - 08_mcp_with_oauth.py: requires OAuth setup
                  # - 15_browser_use.py: requires browser setup
                  # - 16_llm_security_analyzer.py: requires user input
                  EXAMPLES=(
                      "examples/01_standalone_sdk/01_hello_world.py"
                      "examples/01_standalone_sdk/02_custom_tools.py"
                      "examples/01_standalone_sdk/03_activate_skill.py"
                      "examples/01_standalone_sdk/05_use_llm_registry.py"
                      "examples/01_standalone_sdk/07_mcp_integration.py"
                      "examples/01_standalone_sdk/09_pause_example.py"
                      "examples/01_standalone_sdk/10_persistence.py"
                      "examples/01_standalone_sdk/11_async.py"
                      "examples/01_standalone_sdk/12_custom_secrets.py"
                      "examples/01_standalone_sdk/13_get_llm_metrics.py"
                      "examples/01_standalone_sdk/14_context_condenser.py"
                      "examples/01_standalone_sdk/17_image_input.py"
                      "examples/01_standalone_sdk/18_send_message_while_processing.py"
                      "examples/01_standalone_sdk/19_llm_routing.py"
                      "examples/01_standalone_sdk/20_stuck_detector.py"
                      "examples/01_standalone_sdk/21_generate_extraneous_conversation_costs.py"
                      "examples/01_standalone_sdk/22_anthropic_thinking.py"
                      "examples/01_standalone_sdk/23_responses_reasoning.py"
                      "examples/01_standalone_sdk/24_planning_agent_workflow.py"
                  )

                  # Track results
                  FAILED=0
                  PASSED=0
                  FAILED_EXAMPLES=()

                  echo "=========================================="
                  echo "Running ${#EXAMPLES[@]} examples with $LLM_MODEL"
                  echo "=========================================="

                  for example in "${EXAMPLES[@]}"; do
                      echo ""
                      echo "Running: $example"
                      echo "------------------------------------------"
                      
                      # Run example with timeout (5 minutes per example)
                      if timeout 300 uv run python "$example"; then
                          echo "✓ PASSED: $example"
                          PASSED=$((PASSED + 1))
                      else
                          EXIT_CODE=$?
                          echo "✗ FAILED: $example (exit code: $EXIT_CODE)"
                          FAILED=$((FAILED + 1))
                          FAILED_EXAMPLES+=("$example")
                      fi
                  done

                  echo ""
                  echo "=========================================="
                  echo "Test Results Summary"
                  echo "=========================================="
                  echo "Total: ${#EXAMPLES[@]}"
                  echo "Passed: $PASSED"
                  echo "Failed: $FAILED"

                  if [ $FAILED -gt 0 ]; then
                      echo ""
                      echo "Failed examples:"
                      for failed_example in "${FAILED_EXAMPLES[@]}"; do
                          echo "  - $failed_example"
                      done
                      exit 1
                  fi

                  echo ""
                  echo "All examples passed! ✓"

            - name: Comment on PR
              if: always()
              uses: actions/github-script@v7
              with:
                  github-token: ${{ secrets.GITHUB_TOKEN }}
                  script: |
                      const fs = require('fs');
                      const jobStatus = '${{ job.status }}';
                      const runUrl = `${context.serverUrl}/${context.repo.owner}/${context.repo.repo}/actions/runs/${context.runId}`;

                      let message;
                      if (jobStatus === 'success') {
                          message = `## ✅ Examples Test Passed\n\nAll examples ran successfully with \`openhands/claude-haiku-4-5-20251001\`.\n\n[View workflow run](${runUrl})`;
                      } else {
                          message = `## ❌ Examples Test Failed\n\nSome examples failed to run. Please check the logs for details.\n\n[View workflow run](${runUrl})`;
                      }

                      github.rest.issues.createComment({
                          issue_number: context.issue.number,
                          owner: context.repo.owner,
                          repo: context.repo.repo,
                          body: message
                      });
